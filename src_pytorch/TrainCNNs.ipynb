{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home.stud/dobkomar/nodule_detection/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'mtrand' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "from tqdm import tqdm\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time, copy\n",
    "from models_pytorch import CNNT5, CNNT4\n",
    "from vgg_pytorch import vgg11\n",
    "from data_preprocessing.dataloaders_pytorch import LUNA_Dataset\n",
    "from train_tools import train_model\n",
    "\n",
    "LOGS = '/home.stud/dobkomar/NoduleDetection/logs/'\n",
    "\n",
    "\n",
    "def get_free_gpu():\n",
    "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
    "    memory_available = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
    "    print(memory_available)\n",
    "    os.remove('tmp')\n",
    "    return np.argmax(memory_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training parameters configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.0005\n",
    "shuffle_dataset = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/home.stud/dobkomar/data/train_D1.csv')\n",
    "val_data = pd.read_csv('/home.stud/dobkomar/data/test_D1.csv')\n",
    "\n",
    "\n",
    "# IF 2-D DATASET\n",
    "train_dataset = LUNA_Dataset(train_data)\n",
    "val_dataset = LUNA_Dataset(val_data)\n",
    "\n",
    "# ELIF 3-D DATASET\n",
    "# train_dataset = LUNA_Dataset_3D(train_data)\n",
    "# val_dataset = LUNA_Dataset_3D(val_data)\n",
    "\n",
    "\n",
    "train_loader = utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = utils.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# train_iter, val_iter = iter(train_loader), iter(validation_loader)\n",
    "# images_tr, label_tr = train_iter.next()\n",
    "# images_v, labels_v = val_iter.next()\n",
    "\n",
    "# print('TRAIN Images shape on batch size = {}'.format(images_tr.size()))\n",
    "# print('TRAIN Labels shape on batch size = {}'.format(label_tr.size()))\n",
    "# print('TRAIN shape: {}'.format(len(train_loader)))\n",
    "# print('VAL Images shape on batch size = {}'.format(images_v.size()))\n",
    "# print('VAL Labels shape on batch size = {}'.format(labels_v.size()))\n",
    "# print('VAL shape: {}'.format(len(validation_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11178, 11178, 11178, 11178, 395, 395, 395, 395]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:{}\".format(get_free_gpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the models except for VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNT4(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=400, out_features=150, bias=True)\n",
      "  (fc2): Linear(in_features=150, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc_out): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNNT4().to(device)  # HERE YOU CAN PUT ANY NETWORK FROM module models_pytorch.py\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# learning_rate = 0.0005\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.95)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0005, momentum=0.95)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()   # weight=class_weights)\n",
    "\n",
    "dataloaders_dict = {'train': train_loader, 'val': validation_loader}\n",
    "\n",
    "# Train and evaluate\n",
    "model, hist, df_logs = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# Saving results and model\n",
    "df_logs.to_csv('/home.stud/dobkomar/NoduleDetection/logs/models/convnet4_D1.csv', index=False)\n",
    "name = 'convnet4_D1'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, '/home.stud/dobkomar/NoduleDetection/logs/models/' + '{}.pt'.format(name))\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE MODEL \n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "# VGG11\n",
    "feature_extract = False\n",
    "model = vgg11(pretrained=False)\n",
    "set_parameter_requires_grad(model, feature_extract)\n",
    "num_ftrs = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_ftrs, 2)\n",
    "input_size = 32\n",
    "\n",
    "num_features = model.classifier[6].in_features\n",
    "features = list(model.classifier.children())[:-1]   # Remove last layer\n",
    "model = model.to(device)\n",
    "features[0] = nn.Linear(2048, 4096)\n",
    "features.extend([nn.Linear(4096, 2)])    # Add our layer with 4 outputs\n",
    "model.classifier = nn.Sequential(*features)   # Replace the model classifier\n",
    "model.classifier.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU(inplace)\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.3)\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.3)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL \n",
    "\n",
    "# Send the model to GPU\n",
    "model = model.to(device)\n",
    "params_to_update = model.parameters()\n",
    "\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# optimizer = torch.optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dataloaders_dict = {'train': train_loader, 'val': validation_loader}\n",
    "\n",
    "# Train and evaluate\n",
    "model, hist, df_logs = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results and model\n",
    "df_logs.to_csv('/home.stud/dobkomar/NoduleDetection/logs/models/vgg11_D1.csv', index=False)\n",
    "name = 'vgg11_D1'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, '/home.stud/dobkomar/NoduleDetection/logs/' + '{}.pt'.format(name))\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
